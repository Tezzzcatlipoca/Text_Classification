url_dataf<-NULL
url_dataf <- c(url_dataf, unique(job_urls))
length(url_dataf)
length(puesto_dataf)
for (pag in 2:30) {
# Wait up some time in each loop
tiempo<-75+rnorm(1,20,41.5)
Sys.sleep(tiempo)
# Retrieve next page
curr_url<-paste0("https://www.occ.com.mx/empleos/?page=",pag)
rem_browser$navigate(curr_url)
# Get Source code
webpage<-rem_browser$getPageSource()
# Get text data from list variable and transform it into xml_document
web_html<-read_html(webpage[[1]]) # Parse
# Obtener titulos de ofertas laborales
puesto <- html_nodes(web_html,'h2') # Job Titles
# Convertimos la información en texto
puesto_data <- html_text(puesto)
puesto_dataf <- c(puesto_dataf, puesto_data)
# Obtener URLs de cada oportunidad laboral
urls <- web_html %>% html_nodes('a') %>% html_attr("href")
job_urls <- paste0("https://www.occ.com.mx",
urls[grepl(pattern = "/empleo/oferta/",x = urls)])
url_dataf <- c(url_dataf, unique(job_urls))
}
length(url_dataf)
length(puesto_dataf)
togeth<-data.frame(puesto=puesto_dataf,url=url_dataf)
2700/3
# Empleamos la biblioteca para leer códigos web
library('rvest')
#library('httr')
library('curl')
library('RSelenium')
pagina<-read_html(choose.files())
class(pagina)
html_text(pagina)
pagina %>% html_nodes("body") %>% html_text()
pagina %>% html_nodes("a") %>% html_text()
pagina %>% html_nodes("div") %>% html_text()
pagina %>% html_nodes("div") %>% html_text()[1]
ooo<-pagina %>% html_nodes("div") %>% html_text()
ooo[1]
ooo[17]
ooo[52]
library(readr)
mark_all_CR <- function (this_text) {
this_text[this_text==""] <- "\n"
return(this_text)
}
setwd("C:/Users/Tezzz/Documents/Proyectos/DS_Literature/data/top100")
first_file_name <- dir()[1]
file_connection <- file(first_file_name, "r")
file_encoding <- guess_encoding(first_file_name)[1,1][[1]]
file_contents <- readLines(con = file_connection, encoding = file_encoding)
close(file_connection)
ttext <- mark_all_CR(file_contents)
complete_text <- paste0(ttext, collapse = " |°| ")
nchar(complete_text)
substr(complete_text,1,500)
?tidytext
library(tidytext)
?unnest_tokens
library(tokenizers)
?tokenize_ptb
# Get number of unique words (or stems?)
all_tokens <- tokenize_ptb(complete_text,lowercase = TRUE)
class(all_tokens)
length(all_tokens)
nchar(all_tokens)
substr(all_tokens,1,500)
all_tokens
# Get number of unique words (or stems?)
all_tokens <- tokenize_ptb(complete_text,lowercase = TRUE)[[1]]
class(all_tokens)
length(all_tokens)
all_tokens[1:15]
each_token <- unique(all_tokens)
length(each_token)
riqueza_lexica <- length(each_token) # (tokenizer does not stem -alice's, alice-)
each_token
each_token[nchar(each_token)<4]
each_token[nchar(each_token)<3]
each_token[nchar(each_token)<2]
class(ttext)
length(ttext)
ttext[1:15]
all_tokens[1:15]
which(c(TRUE, FALSE))
which(c(TRUE, FALSE,TRUE))
ooo<-all_tokens[1:150]
ooo
ooo %in% c("|°|",".")
which(ooo %in% c("|°|","."))
# Get a map of pauses and syllables (rythm)
# First, where are stops/silences.
stop_characters <- c(",", ";", ".")
ubi <- which(all_tokens %in% stop_characters)
length(ubi)
ubi[1:100]
all_tokens[1:30]
?grepl
grepl("[aeiouy][bcdfghjklmnpqrstvwxz","adventures")
grepl("[aeiouy][bcdfghjklmnpqrstvwxz]","adventures")
grepl("[aeiouy][bcdfghjklmnpqrstvwxz]","dvntrs")
grepl("[aeiouy][bcdfghjklmnpqrstvwxz]","advntrs")
grepl("[aeiouy][bcdfghjklmnpqrstvwxz]","dventrs")
grepl("[aeiouy][bcdfghjklmnpqrstvwxz]","e")
grep("[aeiouy][bcdfghjklmnpqrstvwxz]","adventures")
grep("[aeiouy][bcdfghjklmnpqrstvwxz]","dventures")
regexpr("[aeiouy][bcdfghjklmnpqrstvwxz]","adventures")
gregexpr("[aeiouy][bcdfghjklmnpqrstvwxz]","adventures")
gregexpr("[aeiouy][bcdfghjklmnpqrstvwxz]","dventures")
vowels<-c("a","e","i","o","u","y")
consonants<-c("b","c","d","f","g","h","j","k","l","m","n","p","q","r","s","t","v","w","x","z")
all_vowels<-paste0(vowels,collapse = "")
all_consonants<-paste0(consonants, collapse = "")
all_consonants
gregexpr("[aeiouy][bcdfghjklmnpqrstvwxz]","adventures")
gregexpr("[aeiouy][bcdfghjklmnpqrstvwxz]","adventure")
gregexpr("[aeiouy][bcdfghjklmnpqrstvwxz|^]","adventure")
gregexpr("[aeiouy][bcdfghjklmnpqrstvwxz]|[aeiouy]^","adventure")
gregexpr("[aeiouy][bcdfghjklmnpqrstvwxz]|[aeiouy]$","adventure")
gregexpr("[aeiouy][bcdfghjklmnpqrstvwxz|$]","adventure")
gregexpr("[aeiouy][bcdfghjklmnpqrstvwxz]|[aeiouy]$","adventure")
gregexpr("[aeiouy][bcdfghjklmnpqrstvwxz]|[aeiouy]$","adventures")
tpattern<-paste0("[",all_vowels,"][",all_consonants,"]|[",all_vowels,"]$")
tpattern<-paste0("[",all_vowels,"][",all_consonants,"]|[",all_vowels,"]$",collapse = "")
tpattern
gregexpr(tpattern,"adventures")
tokenize_characters("perro")
tokenize_words("perro")
ooo<-gregexpr(tpattern,"adventures")
ooo
ooo[[1]]
ooo[[1]][1]
length(ooo[[1]])
ooo[[1]]$match.length
ooo$match.length
count_syllables(this_word_array){
vowels<-c("a","e","i","o","u","y")
consonants<-c("b","c","d","f","g","h","j","k","l","m","n","p","q","r","s","t","v","w","x","z")
all_vowels<-paste0(vowels,collapse = "")
all_consonants<-paste0(consonants, collapse = "")
syllable_count<-c()
for (each_word in this_word_array) {
# Use regular expressions to find vowells separated by other characters
tpattern<-paste0("[",all_vowels,"][",all_consonants,"]|[",all_vowels,"]$",collapse = "")
coincidences<-gregexpr(tpattern,each_word)
number_of_syllables<-length(coincidences[[1]])
syllable_count<-c(syllable_count,number_of_syllables)
}
return(syllable_count)
}
?function
count_syllables <- function(this_word_array) {
vowels<-c("a","e","i","o","u","y")
consonants<-c("b","c","d","f","g","h","j","k","l","m","n","p","q","r","s","t","v","w","x","z")
all_vowels<-paste0(vowels,collapse = "")
all_consonants<-paste0(consonants, collapse = "")
tpattern<-paste0("[",all_vowels,"][",all_consonants,"]|[",all_vowels,"]$",collapse = "")
syllable_count<-c()
for (each_word in this_word_array) {
# Use regular expressions to find vowells separated by other characters
coincidences<-gregexpr(tpattern,each_word)
number_of_syllables<-length(coincidences[[1]])
syllable_count<-c(syllable_count,number_of_syllables)
}
return(syllable_count)
}
length(all_tokens)
all_tokens[1]
all_tokens[1:100]
prueba<-all_tokens[1:100]
uuu<-count_syllables(prueba)
uuu
veremos<-data.frame(word=prueba,count=uuu)
View(veremos)
gregexpr(tpattern,"a")
gregexpr(tpattern,"t")
gregexpr(tpattern,"t")[[1]][1]
gregexpr(tpattern,"t")[[1]][1]==-1
a:z
'a':'z'
find_pattern_in_word <- function(this_word, this_pattern) {
coincidences<-gregexpr(tpattern,this_word)
number_found<-length(coincidences[[1]])
if (coincidences[[1]][1]==-1) {
number_found<-0
}
return(number_found)
}
count_syllables_in_array <- function(this_word_array) {
syllable_count<-c()
for (each_word in this_word_array) {
# Use regular expressions to find vowells separated by other characters
tpattern<-paste0("[",all_vowels,"][",all_consonants,"]|[",all_vowels,"]$",collapse = "")
number_of_syllables <- find_pattern_in_word(each_word,tpattern)
syllable_count<-c(syllable_count,number_of_syllables)
}
return(syllable_count)
}
uuu<-count_syllables_in_array(prueba)
uuu
veremos<-data.frame(word=prueba,count=uuu)
View(veremos)
length(each_token)
each_token[1:100]
number_of_syllables <- count_syllables_in_array(each_token)
number_of_syllables
# Get a map of pauses and syllables (rythm)
# First, where are stops/silences.
stop_characters <- c(",", ";", ".")
where_stop_chars <- which(all_tokens %in% stop_characters)
where_stop_chars
number_of_syllables/where_stop_chars
syllables_per_word <- count_syllables_in_array(each_token)
length(where_stop_chars)/length(syllables_per_word)
length(syllables_per_word)/length(where_stop_chars)
sum(syllables_per_word)/length(where_stop_chars)
length(where_stop_chars)
syllables_per_word <- count_syllables_in_array(all_tokens)
sum(syllables_per_word)/length(where_stop_chars)
# Get all possible characters and their possitions
all_case_sens_tokens <- tokenize_ptb(complete_text,lowercase = FALSE)[[1]]
length(all_case_sens_tokens)
head(all_case_sens_tokens)
tail(all_case_sens_tokens)
ooo<-head(all_case_sens_tokens)
ooo
grepl("[A-Z]^",ooo)
grepl("[A-Z]",ooo)
grepl("^[A-Z]",ooo)
# Which start with Uppercase?
where_name_candidate <- grepl("^[A-Z]", all_case_sens_tokens)
name_candidates <- all_case_sens_tokens[where_name_candidate]
length(name_candidates)
head(name_candidates)
head(name_candidates,15)
iii<-grepl("'s",name_candidates)
name_candidates[iii]
iii
sum(iii)
head(name_candidates,15)
iii<-grepl("\'s",name_candidates)
sum(iii)
iii<-grepl("[']s",name_candidates)
sum(iii)
iii<-grepl("['][\w]",name_candidates)
iii<-grepl("['][\\w]",name_candidates)
sum(iii)
iii<-grepl("[\']",name_candidates)
sum(iii)
iii<-grepl("'",name_candidates)
sum(iii)
name_candidates[2]
substr(name_candidates[2],10)
substr(name_candidates[2],10,1)
substr(name_candidates[2],10,2)
substr(name_candidates[2],10,11)
substr(name_candidates[2],10,10)
asc(substr(name_candidates[2],10,10))
library(gtools)
asc(substr(name_candidates[2],10,10))
iii<-grepl("[[:punct:]]",name_candidates)
sum(iii)
head(iii)
grepl("[[:punct:]]",name_candidates[2])
grepl("[^[:alnum:]]s",name_candidates[2])
grepl("[^[:alnum:]]",name_candidates[2])
grepl("[^[:alnum:]]s",name_candidates[1])
name_candidates[1:10]
grepl("[^[:alnum:]]s",name_candidates[1:10])
grepl("[^[:alnum:]]s$",name_candidates[1:10])
grepl("[^[:alnum:]]s$",c("Peter's","Pere-s","Pere-so"))
gsub("[^[:alnum:]]s$","",c("Peter's","Pere-s","Pere-so"))
remove_genitives <- function(this_word_array) {
corrected <- gsub("[^[:alnum:]]s$", "", this_word_array)
return(corrected)
}
find_capitalised_words <- function(this_word_array) {
found_capitalised <- grepl("^[A-Z]", this_word_array)
output <- this_word_array[found_capitalised]
return(output)
}
name_candidates <- find_capitalised_words(all_case_sens_tokens)
head(name_candidates)
head(name_candidates,20)
# Which start with Uppercase?
name_candidates <- remove_genitives(find_capitalised_words(all_case_sens_tokens))
head(name_candidates,20)
data("stop_words")
head(stop_words$word)
ooo<-c("aaa","bbb","ccc")
ooo %in% c("b","bbb")
please_remove <- function(these_words, from) {
found_coincidences <- from %in% these_words
output <- from[!found_coincidences]
return(output)
}
please_remove(these_words = c("pedo","caca"), from = c("hoyo","pedo","asaa","caca"))
name_candidates <- tolower(name_candidates)
name_candidates <- please_remove(these_words = stop_words$word, from = name_candidates)
head(name_candidates)
ooo<-table(name_candidates)
head(ooo)
tail(ooo)
head(sort(ooo,decreasing = True))
head(sort(ooo,decreasing = TRUE))
head(sort(ooo,decreasing = TRUE),30)
head(sort(ooo,decreasing = TRUE),50)
head(sort(ooo,decreasing = TRUE),60)
most_important <- table(name_candidates)
head(most_important)
most_important[most_important>=10]
?table
sort(most_important)
?sort
character_freq_threshold <<- 10
ordered_candidates <- sort(table(name_candidates), decreasing = TRUE)
most_important <- ordered_candidates[ordered_candidates>=character_freq_threshold]
most_important
# Get most important keywords and their locations
all_stems <- tokenize_word_stems(complete_text)
head(all_stems)
select_main <- function(this_word_array, this_threshold) {
ordered_words <- sort(table(this_word_array), decreasing = TRUE)
most_important <- ordered_words[ordered_words>=this_threshold]
return(most_important)
}
most_important <- select_main(name_candidates, character_freq_threshold)
most_important
length(all_stems)
length(all_stems[[1]])
keyword_freq_threshold <<- 10
# Get most important keywords and their locations
all_stems <- tokenize_word_stems(complete_text)[[1]]
all_stems <- please_remove(these_words = stop_words$word, from = all_stems)
keywords <- select_main(all_stems, keyword_freq_threshold)
keywords
dir()
# Set global variables
work_directory <<- "C:/Users/Tezzz/Documents/Proyectos/DS_Literature/"
setwd(work_directory)
dir()
setwd("data")
dir()
setwd("dictionaries")
dir()
first_file_name <- "eng1.txt"
file_connection <- file(first_file_name, "r")
file_encoding <- guess_encoding(first_file_name)[1,1][[1]]
file_contents <- readLines(con = file_connection, encoding = file_encoding)
length(file_connection)
length(file_contents)
head(file_contents)
file_contents[15]
file_contents[35]
file_contents[55]
file_contents[65]
file_contents[64]
file_contents[63]
file_contents[62]
file_contents[61]
file_contents[60]
file_contents[500]
file_contents[501]
file_contents[480]
file_contents[479]
file_contents[478]
file_contents[477]
grepl("[A-Z]", file_contents[477])
grepl("[A-Z]", file_contents[478])
grepl("[A-Z][^a-z]", file_contents[478])
grepl("[A-Z]^[a-z]", file_contents[478])
file_contents[478]
grepl("[A-Z]^[a-z]", file_contents[477])
grepl("[^a-z]", file_contents[477])
grepl("[^a-z]", file_contents[478])
file_contents[478]
grepl("[^a-z]", "Testing This shit")
grepl("[^[a-z]]", "Testing This shit")
grepl("[^[a-z]]", "T")
grepl("[^[a-z]]", "TESSSS")
grepl("[^[a-z]]", "TESSSSa")
grepl("[^[a-z]]", "TESSSSA")
grepl("^[a-z]", "TESSSSA")
grepl("^[a-z]", "TESSSSAa")
grepl("[^a-z]", "TESSSSAa")
grepl("[a-z]", "TESSSSAa")
grepl("[a-z]", "TESSSSA")
?rbind
work_directory <<- "C:/Users/Tezzz/Documents/Proyectos/DS_Literature/data/dictionaries"
setwd(work_directory)
first_file_name <- "eng1.txt"
file_connection <- file(first_file_name, "r")
file_encoding <- guess_encoding(first_file_name)[1,1][[1]]
file_contents <- readLines(con = file_connection, encoding = file_encoding)
close(file_connection)
new_dictionary <- data.frame()
also_copy_next <- FALSE
for (each_line in file_contents) {
if (also_copy_next) {
new_line <- data.frame(Term=each_line)
new_dictionary <- rbind(new_dictionary, new_line)
also_copy_next <- FALSE
}
if (!grepl("[a-z]", each_line)) {
new_line <- data.frame(Term=each_line)
new_dictionary <- rbind(new_dictionary, new_line)
also_copy_next <- TRUE
}
}
each_line
tail(new_dictionary)
tail(new_dictionary,30)
rm(new_dictionary)
install.packages("udpipe")
library(udpipe)
?VectorSource
??VectorSource
library(tm)
uuu<-VectorSource(ttext)
inspect(VCorpus(uuu))
VCorpus(uuu)
corp<-Corpus(VectorSource(ttext))
class(corp)
corp<-tm_map(corp, removeNumbers)
corp<-tm_map(corp, content_transformer(tolower))
corp<-tm_map(corp, removePunctuation)
corp<-tm_map(corp, removeWords, stopwords("english"))
corps<-unlist(as.list(corps))
corp<-unlist(as.list(corp))
?udpipe_download_model
getwd()
lang_model <- udpipe_download_model(language = "english")
dir()
class(lang_model)
head(lang_model)
# identify adjectives and adverbs
udmodel_english <- udpipe_load_model(file = 'english-ewt-ud-2.3-181115.udpipe')
s <- udpipe_annotate(udmodel_english, corpus)
s <- udpipe_annotate(udmodel_english, corp)
class(s)
x <- data.frame(s)
View(x)
dim(x)
length(ttext)
stats <- txt_freq(x$upos)
stats
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "yellow",
main = "UPOS (Universal Parts of Speech)\n frequency of occurrence",
xlab = "Freq")
?barchart
library(lattice)
barchart(key ~ freq, data = stats, col = "yellow",
main = "UPOS (Universal Parts of Speech)\n frequency of occurrence",
xlab = "Freq")
cols(x)
names(x)
head(x)
?udpipe_annotate
s <- udpipe_annotate(udmodel_english, complete_text)
class(s)
x2<-data.frame(s)
head(x2)
stats2 <- txt_freq(x2$upos)
stats2
stats2$key <- factor(stats2$key, levels = rev(stats2$key))
barchart(key ~ freq, data = stats2, col = "yellow",
main = "UPOS (Universal Parts of Speech)\n frequency of occurrence",
xlab = "Freq")
ooo
yyy
if yyy: print(TRUE)
if (yyy): print(TRUE)
exists(yyy)
?exists
exists("yyy")
exists("ooo")
exists("stats")
exists("statst")
?annotate
# identify adjectives and adverbs
text_notes <- annotate_text(ttext)
annotate_text<-function(this_text){
if (!exists("udmodel_english")) {
udmodel_english <<- udpipe_load_model(file = 'english-ewt-ud-2.3-181115.udpipe')
}
output <- data.frame(udpipe_annotate(udmodel_english, this_text))
return(output)
}
# identify adjectives and adverbs
text_notes <- annotate_text(ttext)
head(text_notes)
View(text_notes)
complete_text_2 <- paste0(ttext, collapse = " ")
ttext_2 <- tokenize_sentences(complete_text_2)
text_notes <- annotate_text(ttext_2)
# identify adjectives and adverbs
complete_text_2 <- paste0(ttext, collapse = " ")
ttext_2 <- tokenize_sentences(complete_text_2)
text_notes <- annotate_text(ttext_2)
class(ttext_2)
length(ttext_2)
length(ttext_2[[1]])
ttext_2[[1]][1:15]
ttext_2 <- tokenize_sentences(complete_text_2)[[1]]
text_notes <- annotate_text(ttext_2)
unique(text_notes[text_notes$upos=="PROPN","lemma"])
